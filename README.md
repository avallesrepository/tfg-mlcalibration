Evaluating calibrators for reliable predictions

This study evaluates the performance of various calibration techniques in
 machine learning to ensure reliable probabilistic predictions. Using 44 publicly
 available datasets—both small and medium-sized—the research examines five
 calibration methods: Platt scaling, Beta calibration, Isotonic regression, Bayesian
Binning, and Venn-Abers. These methods are applied to different machine learn
ing model families, including Decision Trees, Ensembles, K-Nearest Neighbors,
 Logistic Regression, Neural Networks, Bayesian Models, and Support Vector Ma
chines. The study assesses their impact on model accuracy, log loss, and expected
 calibration error (ECE).

This repository contains the source code and the statistical results of this research.
